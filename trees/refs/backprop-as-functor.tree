\title{Backprop as Functor: A compositional perspective on supervised learning}
\date{2019-05-01}
\taxon{Reference}
\meta{external}{http://arxiv.org/abs/1711.10455}
\author/literal{Brendan Fong}
\author/literal{David I. Spivak}
\author/literal{Rémy Tuyéras}
\meta{bibtex}{\startverb
 @article
{backprop-as-functor, title={Backprop as Functor: A compositional perspective on supervised learning}, url={http://arxiv.org/abs/1711.10455}, abstractNote={A supervised learning algorithm searches over a set of functions A → B parametrised by a space P to ﬁnd the best approximation to some ideal function f : A → B. It does this by taking examples (a, f (a)) ∈ A × B, and updating the parameter according to some rule. We deﬁne a category where these update rules may be composed, and show that gradient descent—with respect to a ﬁxed step size and an error function satisfying a certain property—deﬁnes a monoidal functor from a category of parametrised functions to this category of update rules. A key contribution is the notion of request function. This provides a structural perspective on backpropagation, giving a broad generalisation of neural networks and linking it with structures from bidirectional programming and open games.}, journal={arXiv:1711.10455 [cs, math]}, author={Fong, Brendan and Spivak, David I. and Tuyéras, Rémy}, year={2019}, month={May},language={en} }
\stopverb}
\subtree{\title{Abstract}
\p{
A supervised learning algorithm searches over a set of functions A → B parametrised by a space P to ﬁnd the best approximation to some ideal function f : A → B. It does this by taking examples (a, f (a)) ∈ A × B, and updating the parameter according to some rule. We deﬁne a category where these update rules may be composed, and show that gradient descent—with respect to a ﬁxed step size and an error function satisfying a certain property—deﬁnes a monoidal functor from a category of parametrised functions to this category of update rules. A key contribution is the notion of request function. This provides a structural perspective on backpropagation, giving a broad generalisation of neural networks and linking it with structures from bidirectional programming and open games.
}}

