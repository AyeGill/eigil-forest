\date{2025-04-06}
\author{eigil-rischel}
\import{macros}
\title{Introduction}
\parent{efr-0001}
\p{
  In several different parts of applied category theory, people have found it useful to study categories whose morphisms describe transformations or functions that take place in two "stages", where the second is dependent on the first, but composes in the other direction---so-called \em{lenses}. In many of these domains, stochastic phenomena play a role. There is a useful generalization of lenses to categories of stochastic maps---the so-called \em{optics} of Riley, \ref{riley-optics}---but this does not accommodate another useful generalization, so called \em{dependent lenses} where not only the backwards process but the set it takes values in is indexed over the base. It has been a long-standing problem to develop a suitable common generalization, "dependent optics", of these two ideas. In this thesis, we solve this problem by developing a theory of \em{Markov fibrations} (\ref{efr-O088}).
}
\p{
  A Markov fibration is a weakening of the notion of (Grothendieck) fibration to include (subject to some assumptions) Markov categories of indexed families of objects (given by deterministic functions #{E \to X}) and compatible stochastic maps (given by commutative squares). The main point of Markov fibrations is that they admit \em{fiberwise opposites}, which generalize the fiberwise opposites of ordinary categories. Just as the fiberwise opposite of the codomain fibration of a finitely complete category describes dependent lenses (see \ref{efr-ZCTD}), the fiberwise opposite of these codomain Markov fibrations give a good notion of stochastic lens (see \ref{efr-K6NM}).
}
\p{
  We will apply our theory chiefly to two problems. First, we will use them to generalize \em{open games} (Hedges, \ref{hedges-towards-compositional-thesis}) to a larger class of interfaces (namely, indexed families of sets) while at the same time considering possibly-stochastic maps. This will allow us to construct the so-called \em{external choice} operator on open games, which describes branching.
}
\p{
  Secondly, we will generalize Myers' categorical dynamical systems theory to allow for stochastic maps in the base. When combined with another generalization of these systems, to general parameterized lenses, this gives a more natural way of modeling certain stochastic dynamical systems, such as those associated to the training dynamics of machine learning models, see eg \ref{efr-RUTY}.
}
\subtree{
  \title{A brief introduction to lenses}
\p{
  If #{\cC} is a category with finite products, the category #{\Lens(\cC)} of \em{lenses} in #{\cC} has objects pairs #{\binom{A}{X}} of objects in #{\cC}, and morphisms #{\binom{A}{X} \lensto \binom{B}{Y}} given by pairs #{f: X \to Y, f^\#: B \times X \to A}. (Note that it is of course the morphisms, not the objects, that are called lenses). One way to generalize this is to ask for #{\cC} to have pullbacks, and consider an object given by a more general map #{A \to X}, and let a map be #{f: X \to Y, f^\#: B \times_Y X \to A} (so that the triangle over #{X} commutes). (Note that this recovers the "simple lenses" when the objects are of the form #{A \times X \to X}). Under the interpretation of a map #{A \to X} as a family of objects #{A_x} indexed over the elements of #{X}, we see this as a "dependent lens" (since #{A} is a "dependent type"). There are various variations of this idea, which ultimately all fit the pattern of taking a Grothendieck fibration #{\cD \to \cC} and forming the fiberwise opposite---for the dependent lenses as above, this is the codomain fibration #{\cC^\to \to \cC}. 
}
\p{
  Meanwhile, another wide-ranging generalization of #{\Lens(\cC)} are the \em{optics} of Riley (\ref{riley-optics}). For #{\cC} a monoidal category, the objects of #{\Optic(\cC)} are again pairs #{\binom{A}{X}} of objects, but now the morphisms are elements of the coend ##{\int^{M \in \cC}\cC(X, M \otimes Y) \times \cC(M \otimes B, A).} That is, to give an optic #{\binom{A}{X} \lensto \binom{B}{Y}} is to give an object #{M \in \cC} and morphisms #{l: X \to M \otimes Y, r: M \otimes B \to A}, up to the equivalence relation generated by, whenever #{s: N \to M, l: X \to N \otimes Y, r: M \otimes B \to A,} identifying the two optics given by #{(M, (s \otimes 1_Y) \circ l, r)} and #{(N, l, r \circ (s \otimes 1_B))}. One can show that, in the case where #{\cC} is a Cartesian monoidal category, this set can be identified with the set of (simple) lenses.
}

\p{
  A vector field on a smooth manifold #{X} is simply a (smooth) section of the tangent bundle #{TX}.
  A section is a bundle map from the trivial bundle #{X \to X}. This gives the idea of considering a \em{parameterized} dynamical system as consisting of a map #{X \to A} to some other smooth manifold, a bundle #{E \to A} (the points of #{E} are the parameters) and a bundle map #{E \times_A X \to TX}.
  As we will see, such a bundle map is exactly a dependent lens in a category of bundles. Building on this idea, Myers (\ref{myers-cst}) described a highly abstract theory of open dynamical systems, given by lenses out of tangent bundles (where the notion of space, bundle and tangent bundle are generalized to any fibration).
}
\p{
   It has been observed that, although a lens from the tangent bundle #{TS \lensto A} may indeed be said to describe an open dynamical system with state space #{S}, the same can be said for a lens of type #{TS \otimes A \lensto I}---although a dynamical system of a different kind (essentially, the first type are the \em{Moore machines}, the second the \em{Mealy machines}---see the introduction to \ref{efr-000D} for more on this). It is natural to consider parameterized maps #{TS \otimes A \lensto B} as a common generalization of these two concepts---and in fact, special cases of this idea, lenses parameterized by tangent bundles, have already been considered many times, for example in the semantics of gradient descent (see \ref{bruno-etal-categorical-learning-2021}, \ref{towards-cybercat}, \ref{backprop-as-functor}).}
\p{
  Since parameterized maps compose in an obvious way, we now have three different notions of morphisms between bundles---lenses, charts, and "generalized systems". These should form some sort of symmetric monoidal triple category, but the right axiomatization of this concept is somewhat elusive.
}
\p{
  Lenses, in their various incarnations, have been widely used in applied category theory. For example, Hedges and his collaborators have developed a compositional approach to game theory (\ref{hedges-string-games}, \ref{hedges-etal-comp-gametheory}, \ref{hedges-etal-institutions}, \ref{hedges-etal-graph-games}, others). Recent work by Hedges and Sakamoto bring this idea to reinforcement learning (\ref{sakamoto-reinforcement-lenses}). We briefly mentioned Myers' categorical systems theory above, and we will see much more of it later. As we discussed above, there is also a literature using lenses to study gradient descent in an abstract sense. This is without even mentioning their original role in the theory of functional programming (this is the origin of the somewhat confusing term \em{lens}), or their prehistory in GÃ¶del's dialectica interpretation---see eg \ref{jules-lenses-blogpost} for a survey of this.
}
}
\subtree{
  \title{
    Dependent Optics
  }
  \p{
    An open game, in the sense of Hedges, has as its interface two objects in the category of lenses (of sets) #{\binom{S}{X}, \binom{R}{Y}} (that is, two pairs of sets). An open game with this interface consists of a set #{\Sigma} of \em{strategies} equipped with a function #{\Sigma \to \Lens(\Set)(\binom{S}{X},\binom{R}{Y}),} and a subset #{Eq(x, k) \subset \Sigma} of \em{equilibrium strategies} for each #{x \in X} and #{k: Y \to R}---note that such an #{x} is exactly a lens #{\binom{*}{*} \lensto \binom{S}{X},} and such a #{k} is exactly a lens #{\binom{R}{Y} \to \binom{*}{*}}. This view of open games makes the composition rule much simpler to define, although we will not delve into the details here, see eg \ref{hedges-etal-comp-gametheory}. The idea is that the open game represents the strategies and preferences of a player or set of players---#{\sigma \in \Sigma} are the possible strategies, #{x \in X} is the information revealed to the player before they make their decision of which moves to make, the resulting #{y \in Y} is the choice the player "sends" in response to #{x}, and the #{r \in R} is the "utility", the value they eventually learn, depending on their choice #{y}, which they have some preferences over.
  }
  \p{
    The above description gives a theory of deterministic games, but of course, it is completely essential for game theory to model both random decisions (mixed strategies) and decisions made under uncertainty. This motivates the replacement of lenses in this definition with optics in a category of probability kernels (the form of the equilibrium relation must be modified as well, see \ref{hedges-etal-bayesian-games}, \ref{towards-cybercat})
  }
  \p{
    In the abstract study of open games, it is desirable to define a "choice" operation #{\oplus} on the objects (the pairs of sets) so that maps into #{\binom{R}{X} \oplus \binom{R'}{X'}} represent players who are faced with some binary choice between #{X} and #{X'}, after which play may proceed for a time in one of two branches. However, this is immediately problematic because the category of lenses do not have coproducts---inside the category of \em{dependent} lenses, however, we can form the coproduct simply as #{R \times X + R' \times X' \to X + X'}---that is, we get a family which is #{R} over #{X}, and #{R'} over #{X'}. This indeed does the job, but for game theory the extension to stochastic maps---optics---is absolutely essential. This motivates the search for a theory of \em{dependent optics}, a common generalization of optics and dependent lenses.
  }
  \p{
    There have been a number of attempts at this---see eg \ref{fibre-optics-2021}, \ref{hedges-braithwaite-dep}, \ref{milewski-polylens}, \ref{vertechi-dep-optics}. While these efforts have to some extent succeeded in describing a theory that is general enough to contain the desired examples, it is generally very ad hoc. The construction of optics as a special case of Vertechi's dependent optics (\ref{vertechi-dep-optics}) gives them as fibre optics with the indexing bicategory being (the delooping of) a monoidal category #{\cM}, but embeds lenses using a bicategory of spans. Thus simple lenses admit two distinct encodings in this theory. Moreover, Vertechi's example of dependent monoidal optics fails to describe a supercategory of #{\Optic(\cC)} when applied to Markov categories, and are thus not suitable for, for example, game theory.
  }
}
\subtree{
  \title{Categorical Cybernetics}
  \p{
    The observation that both open games and machine learning systems can be fruitfully described using lenses, naturally led to the idea that this could be used to develop a more thorough analogy between the two. After all, a machine learning system is also a player in a kind of game (with the objective of minimizing loss). Hedges coined the phrase \em{categorical cybernetics} to describe this locus of ideas (\ref{towards-cybercat}, also used in \ref{sakamoto-reinforcement-lenses}).
  }
  \p{
    Myers' categorical systems theory also describes dynamical systems as lenses. However, where a cybernetic system in the sense of \ref{towards-cybercat} is a parameterized lens #{\Sigma \otimes \arena{\bar{X}}{X} \lensto \arena{\bar{Y}}{Y},} and thus has two different inputs - #{x \in X,} which is the information on which basis they are allowed to choose their decision #{y \in Y,} and #{y^\sharp \in \bar{Y},} which is their "utility", the information they are allowed to care about.
  }
  \p{
    It is straightforward to describe the machine learning part of the categorical cybernetics analogy in terms of Myers' theory---a "learner" is simply a lens of the above form with #{\Sigma} replace with a tangent bundle #{TS}. This idea (described in these terms, although not with the analogy to machine learning) has already been described by \ref{energy-driven-systems}.
    However, for game theory, it is necessary that the output of a given system can be stochastically chosen (a player must be able to randomize their strategy). At the same time, for a machine learning system, the #{x \in X} is usually some sort of sample from a training distribution---ie, it is random. Hence to really describe the training dynamics of machine learning systems using this theory, we are again naturally drawn to consider systems theories (that is, fibrations) which have stochastic maps in the base, not merely the fiber.
  }
}
\subtree{
  \title{Overview and contributions}
  \p{
    We begin the thesis in \ref{efr-POD2} with a review of some preliminary material. This chapter can largely be skipped for readers already familiar with the material (Markov categories, double categories, lenses and optics, fibrations, and categorical dynamical systems theory). The chapter on Markov categories introduces a few novel auxiliary notions, but these can be referred back to as necessary (they are primarily regularity conditions which hold in most Markov categories of interest). 
  }
  \p{
    In \ref{efr-O088}, we give the main technical contribution of the thesis by developing a theory of \em{Markov fibrations}. Their fiberwise opposites generalize both the fiberwise opposites of codomain fibrations (dependent lenses in a classical sense) and optics in Markov categories. \ref{efr-K6NM} provides the statement of the latter (the former is straightforward). We also give a description of monoidal structures on Markov fibrations (which pass to monoidal structures on the resulting categories of optics).
  }
  \p{
    In \ref{efr-000D}, we give a construction of the double category of \em{parameterized morphisms} for a category action (or actegory)---in fact, we do this  internally to any #{2}-category. This has the advantage of allowing us to construct more highly-structured versions of this double category by carrying out the construction internally to higher-structured actions. (Bicategorical versions of #{\Para} are a relatively old idea, and the double categorical version is a straightforward extension which has existed for some time as folklore, but the fully-internal construction here is novel).
  }
  \p{
    In \ref{efr-GFG0}, we apply Markov fibrations to compositional game theory, and solve a longstanding problem by giving a category of stochastic open games with a general "external choice" operator. This construction is largely abstract over the particular category, and so can be applied to different Markov fibrations to describe games defined with different notions of stochasticity. We also review some previous work with Capucci, Hedges, and Gavranovic on abstract constructions of categories of open games (\ref{towards-cybercat}). 
  }
  \p{
    In \ref{efr-ZRUZ}, we describe how to extend Myers' categorical systems theory to Markov fibrations, to give theories of systems which may have stochastic maps not merely in the fiber (as was already described by Myers in \ref{myers-cst}) but also in the base. We also leverage our construction from \ref{efr-000D} to give a description of a \em{symmetric monoidal triple category} whose three types of morphisms, are lenses, charts, and what we call \em{bisystems,} that is morphisms #{TS \otimes A \lensto B} - these are open dynamical systems which have two directions of interaction with the environment.
    We also describe a particular systems theory of smooth manifolds and smooth stochastic maps between them.
  }
}
\transclude{efr-0008}%Acknowledgements