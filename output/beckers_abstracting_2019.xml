<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"><fr:frontmatter><fr:anchor>317</fr:anchor><fr:taxon>Reference</fr:taxon><fr:addr>beckers_abstracting_2019</fr:addr><fr:route>beckers_abstracting_2019.xml</fr:route><fr:source-path>/home/eigil/eigil-forest/trees/refs/beckers_abstracting_2019.tree</fr:source-path><fr:title>Abstracting causal models</fr:title><fr:date><fr:year>2019</fr:year><fr:month>7</fr:month><fr:day>9</fr:day></fr:date><fr:authors><fr:author>Sander Beckers</fr:author><fr:author>Joseph Y. Halpern</fr:author></fr:authors><fr:meta
name="bibtex">@article{beckers_abstracting_2019,
 abstract = {We consider a sequence of successively more restrictive deﬁnitions of abstraction for causal models, starting with a notion introduced by Rubenstein et al. (2017) called exact transformation that applies to probabilistic causal models, moving to a notion of uniform transformation that applies to deterministic causal models and does not allow differences to be hidden by the “right” choice of distribution, and then to abstraction, where the interventions of interest are determined by the map from low-level states to high-level states, and strong abstraction, which takes more seriously all potential interventions in a model, not just the allowed interventions. We show that procedures for combining micro-variables into macro-variables are instances of our notion of strong abstraction, as are all the examples considered by Rubenstein et al.},
 author = {Beckers, Sander and Halpern, Joseph Y.},
 date = {2019-07-09},
 eprint = {1812.03789},
 eprinttype = {arxiv},
 file = {msc1812.03789.pdf:/home/eigil/Dropbox/Matematik/Tekster/msc1812.03789.pdf:application/pdf},
 journaltitle = {{arXiv}:1812.03789 [cs]},
 keywords = {Computer Science - Artificial Intelligence},
 langid = {english},
 title = {Abstracting Causal Models},
 url = {http://arxiv.org/abs/1812.03789},
 urldate = {2021-04-04}
}</fr:meta><fr:meta
name="url">http://arxiv.org/abs/1812.03789</fr:meta><fr:last-changed><fr:date><fr:year>2024</fr:year><fr:month>5</fr:month><fr:day>25</fr:day></fr:date></fr:last-changed></fr:frontmatter><fr:mainmatter></fr:mainmatter><fr:backmatter><fr:contributions></fr:contributions><fr:context></fr:context><fr:related></fr:related><fr:backlinks><fr:tree
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"><fr:frontmatter><fr:anchor>315</fr:anchor><fr:addr>efr-0002</fr:addr><fr:route>efr-0002.xml</fr:route><fr:source-path>/home/eigil/eigil-forest/trees/efr-0002.tree</fr:source-path><fr:title>Bidirectionality in graphical models</fr:title><fr:date><fr:year>2024</fr:year><fr:month>5</fr:month><fr:day>25</fr:day></fr:date><fr:authors><fr:author><fr:link
href="eigil-rischel.xml"
type="local"
addr="eigil-rischel">Eigil Fjeldgren Rischel</fr:link></fr:author></fr:authors><fr:last-changed><fr:date><fr:year>2024</fr:year><fr:month>5</fr:month><fr:day>25</fr:day></fr:date></fr:last-changed></fr:frontmatter><fr:mainmatter><fr:p>
  There is a significant existing literature (see <fr:ref
addr="beckers_abstracting_2019"
href="beckers_abstracting_2019.xml"
taxon="Reference"></fr:ref>, <fr:ref
addr="rischel_compositional_2021"
href="rischel_compositional_2021.xml"
taxon="Reference"></fr:ref>, <fr:ref
addr="rubenstein_causal_2017"
href="rubenstein_causal_2017.xml"
taxon="Reference"></fr:ref>. For a survey see <fr:ref
addr="zennaro_abstraction_2022"
href="zennaro_abstraction_2022.xml"
taxon="Reference"></fr:ref>) studying transformations between structural causal models. These have generally been called something like <fr:em>abstractions,</fr:em> the typical examples being the relationship between a high-level and a low-level model of the same system. However, the precise properties that we should ask for in such a transformation have turned out to be somewhat subtle. The most obvious condition to impose is to ask, for any possible variable we could intervene on, say <fr:tex>X</fr:tex>, and any other variable (or collection of variables) <fr:tex>Y</fr:tex>, that the diagram
  
  <fr:embedded-tex
hash="56d1d417a8c926379522eaa187dcfab6"><fr:embedded-tex-preamble>\usepackage {quiver, amsopn, amssymb, mathrsfs}</fr:embedded-tex-preamble><fr:embedded-tex-body>
     \begin {tikzcd}
     \mathcal {A}_X  \ar [r]  \ar [d] &amp;  \mathcal {A}_Y  \ar [d] \\ 
     \mathcal {A}_{f(X)}  \ar [r] &amp;  \mathcal {A}_{f(Y)}
     \end {tikzcd}
  </fr:embedded-tex-body></fr:embedded-tex>

  of kernels commutes - in other words, given an intervention <fr:em>in the low-level model,</fr:em> the two possible distributions in the high-level model agree.
</fr:p><fr:p>
  The main problem with this condition is that it is simply too strict. We generally can't ask that <fr:em>every</fr:em> low-level intervention is well-modeled by the corresponding high-level intervention. Rather, each high-level intervention corresponds to some distribution of the corresponding low-level variables, and we must ask that a diagram of this form:
  
  <fr:embedded-tex
hash="1dba657b6e6d8d3bd28a2cb8942089a3"><fr:embedded-tex-preamble>\usepackage {quiver, amsopn, amssymb, mathrsfs}</fr:embedded-tex-preamble><fr:embedded-tex-body>
     \begin {tikzcd}
     \mathcal {A}_X  \ar [r]&amp;  \mathcal {A}_Y  \ar [d] \\ 
     \mathcal {A}_{f(X)}  \ar [r]  \ar [u] &amp;  \mathcal {A}_{f(Y)}
     \end {tikzcd}
  </fr:embedded-tex-body></fr:embedded-tex>

  commutes. For example, if the low-level variables are the velocities of all the gas molecules in a container, and the high-level variables are thermodynamic quantities like temperature and pressure, we can't expect that <fr:em>every</fr:em> set of velocities corresponding to a given temperature will lead to a given distribution of outcomes - rather, intervening on the temperature leads to some particular distribution of possible sets of velocities, and this leads to the observed distribution of outcomes.
</fr:p><fr:p>
  The key here is that <fr:em>control information flows in the opposite direction from measurement information</fr:em>. This phenomenon repeats in many examples.
</fr:p></fr:mainmatter></fr:tree></fr:backlinks><fr:references></fr:references></fr:backmatter></fr:tree>